{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwBqVSFr4YfPzeVQhqVAJN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6BOMigru6UK",
        "outputId": "648b403c-e7ef-4f72-d171-e8f5437440df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/wizard_of_oz.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Ow2yf5xZl-",
        "outputId": "906cb128-6c0e-446a-c758-fb70c6c19939"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿  DOROTHY AND THE WIZARD IN OZ\n",
            "\n",
            "  BY\n",
            "\n",
            "  L. FRANK BAUM\n",
            "\n",
            "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
            "\n",
            "  ILLUSTRATED BY JOHN R. NEILL\n",
            "\n",
            "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(set(text))\n",
        "print(chars)\n",
        "print(len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDWly0W82DzK",
        "outputId": "8987e7c8-d7bc-4e66-aba2-f308eed3c68f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
        "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])"
      ],
      "metadata": {
        "id": "3s2SNZxX2WUF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode('hello')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTJw0ahb53lP",
        "outputId": "1764ab36-16b3-4f7b-c81b-5d03fac9be76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[61, 58, 65, 65, 68]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode([44,55,66])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FUnzzhTw6Bjc",
        "outputId": "f78b87cc-4b1e-43c3-8fbc-14401bfdb0a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tbm'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ld4dSJQT6ExJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEBOVtfNQkZQ",
        "outputId": "10d5cbde-4854-4eac-f751-319eb4acc762"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
            "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
            "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
            "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
            "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
            "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[:n]"
      ],
      "metadata": {
        "id": "IbrdxEu03Rfr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "0FIKaJCI7Pa6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f'When input is : {context} --> target is {target}')"
      ],
      "metadata": {
        "id": "fR4cVj2DQ8w9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2c7fba-c25b-4304-8ee8-f53d573c3d13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is : tensor([80]) --> target is 1\n",
            "When input is : tensor([80,  1]) --> target is 1\n",
            "When input is : tensor([80,  1,  1]) --> target is 28\n",
            "When input is : tensor([80,  1,  1, 28]) --> target is 39\n",
            "When input is : tensor([80,  1,  1, 28, 39]) --> target is 42\n",
            "When input is : tensor([80,  1,  1, 28, 39, 42]) --> target is 39\n",
            "When input is : tensor([80,  1,  1, 28, 39, 42, 39]) --> target is 44\n",
            "When input is : tensor([80,  1,  1, 28, 39, 42, 39, 44]) --> target is 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The langauge models needs to be scaled to process and handle large number of input data without affecting its processing time and performance. <br>\n",
        "Here , `batch_size` & `block_size` helps in fast training and good performance."
      ],
      "metadata": {
        "id": "WXXInAhY5-GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'CPU'\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_kTjfMk38Ty",
        "outputId": "859d04f3-81fa-412f-fda3-9bf3742a2335"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.empty(2,3)\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x41tK-Rm7AL1",
        "outputId": "1a460831-0a63-4d2d-d478-b76d48278077"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.6420e-38, 0.0000e+00, 1.6420e-38],\n",
              "        [0.0000e+00, 1.6415e-38, 0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To compare the difference in processing time between CPU and GPU"
      ],
      "metadata": {
        "id": "rNiNz_t2GX6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "torch_rand1 = torch.rand(100,100,100,100).to(device)\n",
        "torch_tand2 = torch.rand(100,100,100,100).to(device)\n",
        "np_rand1 = torch.rand(100,100,100,100)\n",
        "np_rand2 = torch.rand(100,100,100,100)\n",
        "\n",
        "start_time = time.time()\n",
        "rand = (torch_rand1 @ torch_tand2)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f'Elapsed GPU Processing time : {elapsed_time}')\n",
        "\n",
        "start_time = time.time()\n",
        "rand = np.multiply(np_rand1,np_rand2)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f'Elapsed CPU Processing time : {elapsed_time}')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "WKPyOTp4CHIG",
        "outputId": "556e4562-6a23-4dcb-95b0-ecccc61aa197"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport time\\nimport numpy as np\\n\\ntorch_rand1 = torch.rand(100,100,100,100).to(device)\\ntorch_tand2 = torch.rand(100,100,100,100).to(device)\\nnp_rand1 = torch.rand(100,100,100,100)\\nnp_rand2 = torch.rand(100,100,100,100)\\n\\nstart_time = time.time()\\nrand = (torch_rand1 @ torch_tand2)\\nend_time = time.time()\\nelapsed_time = end_time - start_time\\nprint(f'Elapsed GPU Processing time : {elapsed_time}')\\n\\nstart_time = time.time()\\nrand = np.multiply(np_rand1,np_rand2)\\nend_time = time.time()\\nelapsed_time = end_time - start_time\\nprint(f'Elapsed CPU Processing time : {elapsed_time}')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Let's do a recap on torch and its function\n",
        "prob = torch.tensor([0.1,0.9]) #Prob Tensor\n",
        "\n",
        "# 10% or 0.1 --> 0 , 90% or 0.9 --> 1. Each prob points to the probability of that index in the tensor\n",
        "# Draw 5 samples\n",
        "samples = torch.multinomial(prob,num_samples=10,replacement=True)\n",
        "print(samples)"
      ],
      "metadata": {
        "id": "MqyZ93uxHfZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1ecbab-2d3e-45c3-c910-4ee190b5589e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch concat\n",
        "tensor = torch.tensor([1,2,3,4,5])\n",
        "op = torch.cat((tensor,torch.tensor([6,7,8,9])),dim=0)\n",
        "print(op)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPROR7ZzIGg9",
        "outputId": "1a2afa7a-7b57-415c-ac01-495566746dcb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tril - traingular lower\n",
        "out = torch.tril(torch.ones(5,5))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZYn60j-IkW5",
        "outputId": "5b6a00b1-777d-48c2-e784-c967b63cdd0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.],\n",
        "                  [4., 5., 6.],\n",
        "                  [7., 8., 9.]])\n",
        "trildef = torch.tril(a)\n",
        "print(trildef)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ij_JIuGJonA",
        "outputId": "10aba63e-9bad-43da-9ff7-9bcee2f07be9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [4., 5., 0.],\n",
            "        [7., 8., 9.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trilpos = torch.tril(a,diagonal=1)\n",
        "print(trilpos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo1aui-TJu-P",
        "outputId": "c30f5583-3489-492e-f911-1a7a84bfcdbc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 0.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trilneg = torch.tril(a,diagonal=-1)\n",
        "print(trilneg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPkdkmkfKRbh",
        "outputId": "0d97cc7d-4f15-43c5-c8a1-dae1387c57ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [4., 0., 0.],\n",
            "        [7., 8., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.triu(torch.ones(5,5))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RUBGIGbLoq8",
        "outputId": "2e724a8d-e407-4aff-dc23-f2f011c45957"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [0., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.zeros(5,5).masked_fill(torch.tril(torch.ones(5,5))==0,float('-inf'))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1Ks_K8L1HR",
        "outputId": "02dd6dc6-0d53-466c-a4d4-b622a44651b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(out)"
      ],
      "metadata": {
        "id": "x1BEYJF3MAqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0555cbd4-bf59-4cde-d098-2c32de96ef54"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That tensor is a Causal Attention Mask, crucial in LLMs (like GPT) for autoregressive text generation, ensuring the model predicts the next word by only looking at past words, not future ones, by setting future attention scores to negative infinity (which softmax turns to zero), preventing \"cheating\" and enabling sequential prediction. Its purpose is to enforce causality, simulating how humans read and write one word at a time, making the LLM generate coherent, forward-flowing text.\n",
        "<br>\n",
        "\n",
        "Why it's Important (Purpose):<br>\n",
        "- Autoregressive Generation: LLMs generate text token by token (word by word). To predict the next token, the model needs context from the previous tokens. This mask ensures it doesn't see future tokens, forcing it to learn dependencies correctly.\n",
        "Prevents Information Leakage: Without the mask, a token could \"look ahead\" at the answer, which would make training easy but useless for real-world generation where future context isn't available.\n",
        "- Enforces Sequential Learning: It mimics reading from left to right (or right to left, depending on direction), allowing the model to build understanding incrementally.\n",
        "How it Works (Mechanism)\n",
        "Attention Scores: In self-attention, a score is calculated for how much each token should attend to every other token.\n",
        "Mask Application: The mask (your tensor) is added to these scores before the softmax function.\n",
        "- Softmax Effect:\n",
        "e^(-∞) (e to the power of negative infinity) is effectively zero.\n",
        "When the mask applies -inf to future positions, their attention scores become zero after softmax.\n",
        "This means those future tokens get zero probability and don't influence the current token's representation. <br>\n",
        "\n",
        "Example Breakdown (Your Tensor):<br>\n",
        "- tensor([[0., \"-inf,\" \"-inf,\" \"-inf,\" \"-inf], - The first token (index 0) can only attend to itself (the 0.), not future ones.\n",
        "- \" [0., 0., \"-inf,\" \"-inf,\" \"-inf], - The second token (index 1) can attend to itself and the first, but not the third, fourth, or fifth.\n",
        "- ...and so on, creating a lower triangular matrix of allowed attention, allowing only past and current context."
      ],
      "metadata": {
        "id": "0a1jOB9itM6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.zeros(2,3,4)\n",
        "out = input.transpose(0,2)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtXj9kcor5BJ",
        "outputId": "5a8f5c69-07b2-46e9-fec3-f439835a4286"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.tensor([1,2,3])\n",
        "t2 = torch.tensor([4,5,6])\n",
        "t3 = torch.tensor([7,8,9])\n",
        "\n",
        "stack = torch.stack([t1,t2,t3])\n",
        "stack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0AtL-64xsfS",
        "outputId": "6858a6ba-d68f-45eb-b718-bc462b1bad4c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6],\n",
              "        [7, 8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "sample = torch.tensor([10.,10.,10.])\n",
        "linear = nn.Linear(3,3,bias=False)\n",
        "print(linear(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxcatLMCyuQm",
        "outputId": "db033a08-633a-447c-c8dc-92de47dd2c14"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-3.9351, -5.9054, -5.1283], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "t1 = torch.tensor([1.0,2.0,3.0])\n",
        "softmaxop = F.softmax(t1,dim=0)\n",
        "\n",
        "print(softmaxop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIYRZRMyzF0f",
        "outputId": "005295b3-0ae6-4f85-86a7-f893e15b0950"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2],[3,4],[5,6]])\n",
        "b = torch.tensor([[7,8,9],[10,11,12]])\n",
        "\n",
        "print(torch.matmul(a,b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDmrr6_J53I_",
        "outputId": "e97d7614-ff45-4e58-a0a5-d06be8422420"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 27,  30,  33],\n",
            "        [ 61,  68,  75],\n",
            "        [ 95, 106, 117]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int64 = torch.randint(1,(3,2))\n",
        "float32 = torch.rand(2,3)\n",
        "\n",
        "result = torch.matmul(int64,float32)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "kkn5l3xV-dzR",
        "outputId": "4b93b78f-6f06-4ee2-e06c-e38ba2a9dd24"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected m1 and m2 to have the same dtype, but got: long int != float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-52667753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfloat32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: long int != float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpCXLOhG_zuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}